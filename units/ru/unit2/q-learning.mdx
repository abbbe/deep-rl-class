# Введение Q-Обучение [[q-learning]]
## Что такое Q-Обучение? [[what-is-q-learning]]

Q-Обучение - это **метод обучения вне политики основанный на ценности, который использует TD-подход для обучения функции ценности действия:**

- *Вне политики*: мы поговорим об этом в конце данного раздела.
- *Метод, основанный на ценности*: находит оптимальную политику косвенно, обучая функцию ценности состояния или функцию ценности действия, которая определяет **ценность каждого состояния или каждой пары состояние-действие.**
- *TD-подход:* **обновление функции ценности действия на каждом шаге, а не в конце эпизода.**

**Q-Обучение - это алгоритм, который мы используем для обучения нашей Q-функции** - функции ценности действия**, определяющей ценность нахождения в определенном состоянии и выполнения определенного действия в этом состоянии.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg" alt="Q-function"/>
  <figcaption>При заданном состоянии и действии наша Q-функция возвращает ценность состояния и действия (также называемую Q-значением)</figcaption>
</figure>

**Q происходит от "Качества (Quality)" (ценности) данного действия в данном состоянии.**

Давайте вспомним, в чем разница между ценностью и вознаграждением:

- *Ценность состояния* или *пары "состояние-действие"** - это ожидаемое кумулятивное вознаграждение, которое получит наш агент, если он начнет действовать в этом состоянии (или паре "состояние-действие"), а затем будет действовать в соответствии со своей политикой.
- *Вознаграждение* - это **обратная связь, которую я получаю от окружающей среды** после выполнения действия в каком-либо состоянии.

Внутренне наша Q-функция кодируется ** Q-таблицей - таблицей, каждая ячейка которой соответствует значению пары "состояние-действие".** Рассматривайте эту Q-таблицу как **память или шпаргалку нашей Q-функции.**

Рассмотрим на примере лабиринта.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg" alt="Пример с лабиринтом"/>

Q-таблица инициализируется. Поэтому все значения = 0. Эта таблица **содержит для каждого состояния и действия соответствующие значения ценности "состояние-действие "**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg" alt="Пример с лабиринтом"/>

HЗдесь мы видим, что значение **состояния-действия в начальном состоянии и при движении вверх равно 0:**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg" alt="Пример с лабиринтом"/>

Итак: Q-функция использует Q-таблицу, **в которой хранится значение каждой пары "состояние-действие".** При заданном состоянии и действии **наша Q-функция будет искать значение в своей Q-таблице.**

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" alt="Q-function"/>
</figure>

Напомним, что *Q-Обучение* **это алгоритм RL, который:**

- Обучает *Q-функцию* (функцию ценности состояния**), которая внутренне представляет собой **Q-таблицу, содержащую все значения пары "состояние-действие ".**
- При задании состояния и действия наша Q-функция **ищет в своей Q-таблице соответствующее значение.**
- Когда обучение завершено, **мы имеем оптимальную Q-функцию, что означает, что мы имеем оптимальную Q-таблицу.**
- И если у нас **оптимальная Q-функция**, то у нас **оптимальная политика**, поскольку мы **знаем, какое действие лучше предпринять в каждом состоянии.**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" alt="Связь ценности и политики"/>


Вначале **наша Q-таблица бесполезна, поскольку она дает произвольные значения для каждой пары "состояние - действие"** (чаще всего мы инициализируем Q-таблицу в 0). По мере исследования агентом окружающей среды и обновления Q-таблицы она будет давать нам все лучшее и лучшее приближение** к оптимальной политике.

<figure class="image table text-center m-0 w-full">
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg" alt="Q-Обучение/>
  <figcaption>Здесь мы видим, что в процессе обучения наша Q-таблица становится лучше, так как благодаря ей мы можем знать ценность каждой пары "состояние-действие".</figcaption>
</figure>

Теперь, когда мы поняли, что такое Q-Обучение, Q-функция и Q-таблица, **погрузимся глубже в алгоритм Q-Обучения**.

## Алгоритм Q-Обучения [[q-learning-algo]]

Это псевдокод Q-Обучения; давайте изучим каждую часть и **посмотрим, как это работает на простом примере, прежде чем реализовывать его**. Не пугайтесь, все проще, чем кажется! Мы рассмотрим каждый шаг.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Обучение"/>

### Шаг 1: Инициализируем Q-таблицу [[step1]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg" alt="Q-Обучение"/>


Нам необходимо инициализировать Q-таблицу для каждой пары "состояние-действие". **В большинстве случаев мы инициализируем значение 0.**

### Шаг 2: Выбор действия с помощью эпсилон-жадной стратегии [[step2]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg" alt="Q-Обучение"/>


Эпсилон-жадная стратегия - это политика, позволяющая найти компромисс между исследованием и использованием.

Идея заключается в том, что при начальном значении ɛ = 1,0:

- *С вероятностью 1 - ɛ* : мы осуществляем **использование** (т.е. наш агент выбирает действие с наибольшей ценностью пары состояние-действие).
- С вероятностью ɛ: **мы проводим исследование** (пробуем случайное действие).

В начале обучения **вероятность проведения исследования будет огромной, так как ɛ очень высока, поэтому большую часть времени мы будем проводить исследование**. Но по мере продолжения обучения и, соответственно, улучшения нашей **Q-таблицы в своих оценках, мы постепенно уменьшаем значение эпсилон**, так как нам требуется все меньше и меньше исследований и все больше использования.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg" alt="Q-Обучение"/>


### Шаг 3: Выполнить действие At, получить вознаграждение Rt+1 и следующее состояние St+1 [[step3]]

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg" alt="Q-Обучение"/>

### Шаг 4: Обновление Q(St, At) [[step4]]

Помните, что в TD Обучение мы обновляем нашу политику или функцию ценности (в зависимости от выбранного нами метода RL) **после одного шага взаимодействия**.

Для получения TD-цели **мы использовали непосредственное вознаграждение \\(R_{t+1}\\) плюс дисконтированная ценность следующего состояния**, вычисленная путем нахождения действия, которое максимизирует текущую Q-функцию в следующем состоянии. (Мы называем это бутстрапом).

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg" alt="Q-Обучение"/>

Поэтому наша \\(Q(S_t, A_t)\\) **обновленная формула выглядит следующим образом:**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg" alt="Q-Обучение"/>


Это означает, что для обновления \\(Q(S_t, A_t)\\):

- Нам необходимо \\(S_t, A_t, R_{t+1}, S_{t+1}\\).
- Для обновления Q-значения при заданной паре "состояние-действие" мы используем TD-цель.

Как сформировать TD-цель?
1. Мы получаем вознаграждение \\(R_{t+1}\\) после выполнения действия \\(A_t\\).
2. Чтобы получить **лучшее значение пары состояние-действие** для следующего состояния, мы используем жадную политику для выбора следующего наилучшего действия. Обратите внимание, что это не эпсилон-жадная политика, она всегда будет выбирать действие с наибольшей ценностью "состояние-действие".

Затем, когда обновление этого Q-значения завершено, мы переходим в новое состояние и снова выбираем действие **используя эпсилон-жадную политику.**

**Именно поэтому мы говорим, что Q-Обучение - это алгоритм, работающий вне политики.**

## Вне политики (Off-policy) и в соответствии с политикой (On-policy) [[off-vs-on]]

Разница едва заметна:

- *Вне политики (Off-policy)*: использование **другой политики для действия (инференса) и обновления (обучения)**.

Например, в Q-Обучении эпсилон-жадная политика (политика действия) отличается от жадной политики, которая **используется для выбора наиболее ценного для следующего состояния действия для обновления нашего Q-значения (политика обновления)**.


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg" alt="Вне политики/В соответствии с политикой"/>
  <figcaption>Acting Policy</figcaption>
</figure>

Это отличается от той политики, которую мы используем в ходе обучения:


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg" alt="Вне политики/В соответствии с политикой"/>
  <figcaption>Updating policy</figcaption>
</figure>

- *В соответствии с политкой (On-policy):* использование **одной и той же политики для выбора действий и обновления.**

Например, в Sarsa, другом алгоритме, основанном на ценности, **эпсилон-жадная политика выбирает следующую пару "состояние-действие", а не жадная политика.**


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg" alt="Вне политики/В соответствии с политикой"/>
    <figcaption>Sarsa</figcaption>
</figure>

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg" alt="Вне политики/В соответствии с политикой"/>
</figure>
