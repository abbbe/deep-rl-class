# Промежуточный итог [[mid-way-recap]]

Прежде чем погрузиться в Q-Обучение, давайте подведем итоги того, что мы только что узнали.

У нас есть два типа функций, основанных на ценности:

- Функция ценности состояния: возвращает ожидаемую доходность, если **агент стартует в заданном состоянии и в дальнейшем всегда действует в соответствии с данной политикой.**
- Функция ценности действия: возвращает ожидаемую доходность, если **агент стартует в заданном состоянии, предпринимает заданное действие в этом состоянии** и затем вечно действует в соответствии с политикой
- В методах, основанных на ценности, вместо того чтобы обучать политику, **мы определяем ее вручную** и обучаемся функции ценности. Если у нас есть оптимальная функция ценности, то у нас **будет и оптимальная политика.**

Существует два типа методов обучения политики для функции ценности:

- При использовании *метода Монте-Карло* мы обновляем функцию ценности на основе полного эпизода, и поэтому **используем фактическую дисконтированную доходность этого эпизода.**
- With *the TD Learning method,* we update the value function from a step, replacing the unknown \\(G_t\\) with **an estimated return called the TD target.**
- При использовании *метода обучения с учётом временных различий (TD Learning)* мы обновляем функцию ценности на основе шага, заменяя неизвестную \\(G_t\\) на **оцененную доходность, называемую TD target.**


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg" alt="Summary"/>
