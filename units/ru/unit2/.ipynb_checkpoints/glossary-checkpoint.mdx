# Глоссарий [[glossary]]

Это глоссарий, созданный сообществом. Вклад в его создание приветствуется!


### Стратегии поиска оптимальной политики

- **Методы, основанные на политике.** Политика обычно обучается с помощью нейронной сети, которая выбирает, какое действие следует предпринять, учитывая состояние. В этом случае нейронная сеть возвращает агенту действие, которое он должен предпринять, а не использует функцию ценности действия. В зависимости от опыта, полученного в среде, нейронная сеть будет перенастраиваться и выдавать лучшие действия.

- **Методы, основанные на ценности.** В этом случае функция ценности обучается для вывода ценности состояния или пары "состояние-действие", которые будут представлять нашу политику. Однако это значение не определяет, какие действия должен предпринять агент. Напротив, нам необходимо определить поведение агента с учетом значения возвращаемого функции ценности. Например, мы можем принять решение о том, что агент будет выполнять действие, которое всегда приводит к наибольшему вознаграждению (Жадная Политика). Таким образом, Жадная Политика  (или любое другое решение, принимаемое пользователем) использует значения функции ценности для принятия решения о действиях.

### Среди методов, основанных на ценности, можно выделить две основные стратегии

- **Функция ценности состояния.** Для каждого состояния функция ценности состояния - это ожидаемая доходность, если агент начинает действовать в этом состоянии и следует данной политике до конца.
- **Функция ценности действия.** В отличие от функции ценности состояния, функция ценности действия рассчитывает для каждой пары "состояние - действие" ожидаемую доходность, если агент начнет действовать в этом состоянии, предпримет это действие и затем будет следовать этой политике всегда.

### Эпсилон-жадная стратегия:

- Общая стратегия, используемая в обучении с подкреплением, которая предполагает баланс между исследованием и использованием.
- Выбор действия с наибольшим ожидаемым вознаграждением с вероятностью 1-epsilon.
- Выбор случайного действия с вероятностью epsilon.
- Эпсилон обычно уменьшается со временем, чтобы сместить фокус в сторону использования.

### Жадная стратегия:

- Предполагает постоянный выбор действия, которое, как ожидается, приведет к наибольшему вознаграждению, исходя из текущих знаний о среде. (Только использование)
- Всегда выбирается действие с наибольшим ожидаемым вознаграждением.
- Не включает в себя никаких исследований.
- Может оказаться невыгодной в условиях неопределенности или неизвестности оптимальных действий.

### Алгоритмы вне политики и в соответсвии с политикой

- **Алгоритмы вне политики:** В момент обучения и в момент инференса используется разная политика
- **Алгоритмы с соответствии с политикой:** Одна и та же политика используется во время обучения и инференса

### Стратегии обучения по методу Монте-Карло и по методу временных различий

- **Метод Монте-Карло (MC):** Обучение в конце эпизода. При использовании метода Монте-Карло мы ждем окончания эпизода, а затем обновляем функцию ценности (или функцию политики) на основе полного эпизода.

- **Метод с учётом временныз различий (TD):** Обучение на каждом шаге. При использовании метода обучения с учетом временных различий мы обновляем функцию ценности (или функцию политики) на каждом шаге, не ожидая завершения полного эпизода.

Если вы хотите улучшить курс, вы можете [открыть Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)

Создание данного глоссария стало возможным благодаря:

- [Рамон Руэда](https://github.com/ramon-rd)
- [Хасаринду Перера](https://github.com/hasarinduperera/)
- [Аркадий Архангородский](https://github.com/arkadyark/)
