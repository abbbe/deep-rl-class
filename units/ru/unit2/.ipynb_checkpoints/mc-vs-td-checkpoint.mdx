# Монте-Карло против Обучения с учётом временных различий [[mc-vs-td]]

Последнее, что необходимо обсудить перед погружением в Q-Обучение это две стратегии обучения.

Помните, что RL-агент **обучается, взаимодействуя с окружающей средой.**Идея заключается в том, что **с учетом полученного опыта и вознаграждения агент обновляет свою функцию ценности или политику.**

Обучение методом Монте-Карло и обучение с учётом временных различий - это две разные **стратегии обучения функции ценности или функции политики.** Обе они **используют опыт для решения проблемы RL.**

С одной стороны, Монте-Карло использует **целый эпизод для получения опыта перед обучением.** C другой стороны, Учёт временных различий использует **только шаг ( \\(S_t, A_t, R_{t+1}, S_{t+1}\\) ) для обучения.**

Мы объясним их **на примере метода, основанного на ценности.**

## Монте-Карло: обучение в конце эпизода [[monte-carlo]]

Монте-Карло дожидается конца эпизода, вычисляет \\(G_t\\\) (доходность) и использует ее в качестве **цели для обновления \\\(V(S_t)\\\).**

So it requires a **complete episode of interaction before updating our value function.**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg" alt="Monte Carlo"/>


Если мы рассмотрим пример:

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg" alt="Monte Carlo"/>


- Мы всегда начинаем эпизод **с одной и той же начальной точки.**
- **Агент совершает действия, используя политику**. Например, используя эпсилон-жадную стратегию, политику, чередующую исследование (случайные действия) и использование.
- Мы получаем **вознаграждение и следующее состояние.**
- Мы завершаем эпизод, если кошка съедает мышь или если мышь перемещается на расстояние > 10 шагов.

- В конце эпизода **у нас есть список кортежей Состояние, Действие, Вознаграждение и Следующее состояние**.
Например, [[Состояние 3 нижняя клетка, переход налево, +1, Состояние 2 нижняя клетка], [Состояние 2 нижняя клетка, переход налево, +0, Состояние 1 нижняя клетка]...].

- **Агент суммирует общее вознаграждение \\(G_t\\)** (чтобы понять, насколько хорошо он справился с задачей).
- Затем он **обновляет \\(V(s_t)\\)) основываясь на формуле**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg" alt="Monte Carlo"/>

- Затем **начинает новую игру с этими новыми знаниями**

Выполняя все больше и больше эпизодов, **агент будет учиться играть все лучше и лучше.**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg" alt="Monte Carlo"/>

Например, если мы обучаем функцию ценности состояния методом Монте-Карло:

- Мы инициализируем нашу функцию стоимости **так, чтобы она имела доходность (return) 0 для каждого состояния**
- Наша скорость обучения (lr) равна 0.1, а наш дисконт равен 1 (= без дисконтирования)
- Наша мышь **исследует окружающую среду и выполняет случайные действия**


  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4.jpg" alt="Monte Carlo"/>


- Мышь сделала более 10 шагов, следовательно, эпизод заканчивается.

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4p.jpg" alt="Monte Carlo"/>



- У нас есть список состояние, действие, вознаграждение, следующее_состояние (next_state), **нам нужно вычислить доходность (return) \\\(G{t=0}\\\)**.

\\(G_t = R_{t+1} + R_{t+2} + R_{t+3} ...\\) (для простоты мы не дисконтируем вознаграждение)

\\(G_0 = R_{1} + R_{2} + R_{3}…\\)

\\(G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0\\)

\\(G_0 = 3\\)

- Теперь мы можем вычислить **новое** \\(V(S_0)\\):

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg" alt="Monte Carlo"/>

\\(V(S_0) = V(S_0) + lr * [G_0 — V(S_0)]\\)

\\(V(S_0) = 0 + 0.1 * [3 – 0]\\)

\\(V(S_0) = 0.3\\)


  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg" alt="Monte Carlo"/>


## Обучение с учётом временных различий: обучение на каждом шаге [[td-learning]]

**Временное различие (TD), с другой стороны, ожидает только одного взаимодействия (одного шага) \\(S_{t+1}\\)** для формирования TD-target и обновления \\(V(S_t)\\) используя \\(R_{t+1}\\) и \\( \gamma * V(S_{t+1})\\).

Идея **TD заключается в обновлении \\(V(S_t)\\) на каждом шаге.**

Но поскольку мы не прошли весь эпизод, у нас нет \\(G_t\\) (ожидаемой доходности).  Вместо этого **мы оцениваем \\(G_t\\) путем сложения \\(R_{t+1}\\) и дисконтированной стоимости следующего состояния.**

Это называется бутстреппингом. Он называется так **потому что TD основывает свое обновление частично на существующей оценке \\(V(S_{t+1})\\), а не на полной выборке \\(G_t\\).**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg" alt="Временные различия"/>


Этот метод называется TD(0) или **одношаговый (one-step) TD (обновление функции ценности после любого отдельного шага).**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg" alt="Временные различия"/>

Если мы рассмотрим тот же пример,

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2.jpg" alt="Временные различия"/>

- Мы инициализируем нашу функцию ценности таким образом, чтобы для каждого состояния она возвращала доходность 0.
- Наш коэффициент обучения (lr) равен 0,1, а коэффициент дисконтирования равен 1 (без дисконтирования).
- Наша мышь начинает исследование окружающей среды и совершает случайное действие: **пойти налево**
- Она получает вознаграждение  \\(R_{t+1} = 1\\), так как **съела кусочек сыра**


  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg" alt="Временные различия"/>


  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg" alt="Временные различия"/>

Теперь мы можем обновить  \\(V(S_0)\\):

Новое  \\(V(S_0) = V(S_0) + lr * [R_1 + \gamma * V(S_1) - V(S_0)]\\)

Новое \\(V(S_0) = 0 + 0.1 * [1 + 1 * 0–0]\\)

Новое \\(V(S_0) = 0.1\\)

Таким образом, мы только что обновили нашу функцию ценности для Состояния 0.

Теперь мы **продолжаем взаимодействовать с этой средой с помощью нашей обновленной функции ценности.**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg" alt="Temporal Difference"/>

  Резюмируем:

  - With *Monte Carlo*, we update the value function from a complete episode, and so we **use the actual accurate discounted return of this episode.**
  - With *TD Learning*, we update the value function from a step, and we replace \\(G_t\\), which we don't know, with **an estimated return called the TD target.**

  - При использовании метода *Монте-Карло* мы обновляем функцию ценности от полного эпизода, и поэтому **используем фактическую точную дисконтированную доходность этого эпизода.**
  - При использовании метода *TD Обучении* мы обновляем функцию стоимости от шага и заменяем \\(G_t\\), которая нам неизвестна, на **оценочную доходность, называемую TD target.**

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg" alt="Summary"/>
