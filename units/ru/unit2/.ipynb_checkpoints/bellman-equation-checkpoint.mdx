# Уравнение Беллмана: упрощение оценки ценности [[bellman-equation]]

Уравнение Беллмана **упрощает расчет ценности состояния или пары действия-состояния.**


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg" alt="Bellman equation"/>

Из того, что мы узнали ранее, мы знаем, что если мы вычислили \\\(V(S_t)\\\) (ценность состояния), то мы должны вычислить доходность, начиная с этого состояния, и затем следовать этой политике до бесконечности. **(Политика, которую мы определили в следующем примере, является жадной политикой; для упрощения мы не дисконтируем вознаграждение).**

Поэтому, чтобы вычислить \\\(V(S_t)\\\), нужно вычислить сумму ожидаемых вознаграждений. Следовательно:

<figure>
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg" alt="Bellman equation"/>
  <figcaption>Расчет ценности состояния 1: сумма вознаграждений, если бы агент начал действовать в этом состоянии и затем следовал жадной политике (предпринимал действия, приводящие к наилучшим значениям ценности состояния) на всех временных шагах.</figcaption>
</figure>

Тогда для расчета \\\(V(S_{t+1})\\\) нам необходимо рассчитать доходность, начиная с этого состояния \\\(S_{t+1}\\).

<figure>
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg" alt="Bellman equation"/>
  <figcaption>Расчет ценности состояния 2: сумма вознаграждений <b>если агент начнет действовать в этом состоянии</b>, и затем будет следовать <b>политике на протяжении всех временных шагов.</b></figcaption>
</figure>

Вы, наверное, заметили, что мы повторяем вычисление ценности различных состояний, что может быть утомительным, если это нужно делать для каждого значения ценности состояния или пары состояние-действие.

Вместо того чтобы вычислять ожидаемую доходность для каждого состояния или каждой пары "состояние-действие", **мы можем использовать уравнение Беллмана** (подсказка: если вы знаете, что такое динамическое программирование, то это очень похоже! если не знаете, то не переживайте).

Уравнение Беллмана - это рекурсивное уравнение, которое работает следующим образом: вместо того чтобы начинать для каждого состояния с начала и вычислять доходность, мы можем рассматривать ценность любого состояния как:

**Непосредственное вознаграждение (immediate reward)  \\(R_{t+1}\\)  + дисконтированная ценность последующего состояния ( \\(gamma * V(S_{t+1}) \\) ) .**

<figure>
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg" alt="Bellman equation"/>
</figure>


Если вернуться к нашему примеру, то можно сказать, что ценность состояния 1 равна ожидаемой кумулятивной доходности, если мы начнем в этом состоянии.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg" alt="Bellman equation"/>


Расчет ценности состояния 1: сумма вознаграждений **если агент начал действовать в этом состоянии 1** и затем следовал **политике в течение всех временных шагов**.

Это эквивалентно \\(V(S_{t})\\)  = Непосредственное вознаграждение \\(R_{t+1}\\)  + Дисконтированная стоимость следующего состояния  \\(\gamma * V(S_{t+1})\\)

<figure>
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman6.jpg" alt="Bellman equation"/>
  <figcaption>FДля упрощения мы не делаем дисконтирования, поэтому gamma = 1.</figcaption>
</figure>

В целях простоты мы не будем дисконтировать, поэтому гамма = 1.
Но пример с гаммой = 0,99 будет рассмотрен в разделе Q-Обучение данного блока.

- Ценность \\(V(S_{t+1}) \\)  = Непосредственное вознаграждение  \\(R_{t+2}\\)  +Дисконтированная стоимость следующего состояния ( \\(gamma * V(S_{t+2})\\) ).
- И так далее.




Напомним, что идея уравнения Беллмана заключается в том, что вместо вычисления каждой величины как суммы ожидаемых доходов, **что является длительным процессом**, мы вычисляем величину как **сумму непосредственного вознаграждения + дисконтированную стоимость последующего состояния**.

Прежде чем перейти к следующему разделу, подумайте о роли gamma в уравнении Беллмана. Что произойдет, если значение gamma будет очень малым (например, 0,1 или даже 0)? Что произойдет, если значение будет равно 1? Что произойдет, если значение будет очень большим, например, миллион?
