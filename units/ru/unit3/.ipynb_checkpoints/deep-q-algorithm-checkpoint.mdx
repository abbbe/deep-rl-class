# The Deep Q-Learning Algorithm [[deep-q-algorithm]]

Мы узнали, что Глубоким Q-Обучением **используется глубокая нейронная сеть для аппроксимации различных Q-значений для каждого возможного действия в состоянии** (оценка функции ценности состояния).

Разница заключается в том, что на этапе обучения вместо непосредственного обновления Q-значения пары "состояние-действие", как это было сделано в Q-Обучении:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg" alt="Q Loss"/>

при Глубоком Q-Обучении мы создаем **функцию потерь, которая сравнивает наше предсказание Q-значения с Q-target и использует градиентный спуск для обновления весов нашей Глубокой Q-сети, чтобы лучше аппроксимировать наши Q-значения**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg" alt="Q-target"/>

Алгоритм тренировки с Глубоким Q-Обучением состоит из *двух этапов*:

- **Выборка (Sampling)**: мы выполняем действия и **сохраняем наблюдаемые кортежи опыта в памяти воспроизведения**.
- **Обучение**: Случайным образом выбираем **маленький батч кортежей и обучаемся на этом батче, используя градиентный спуск для обновления весов**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg" alt="Выборка Обучение"/>

Это не единственное отличие от Q-Обучения. Глубокому Q-Обучению **может быть свойственна нестабильность**, в основном из-за сочетания нелинейной функции Q-значения (нейронная сеть) и бутстреппинга (когда мы обновляем цели по существующим оценкам, а не по фактической полной доходности).

Для стабилизации обучения мы реализуем три различных решения:
1. *Повтор опыта* для более **эффективного использования опыта**.
2. *Фиксированный Q-target* **для стабилизации обучения**.
3. *Двойное Глубокое Q-Обучение*, для **решения проблемы переоценки Q-значений**.

Давайте рассмотрим их!

## Повторение опыта для более эффективного использования опыта [[exp-replay]]

Зачем мы делаем воспроизведение памяти?

Воспроизведение опыта в Глубоким Q-Обучением имеет две функции:

1. **Более эффективное использование опыта, полученного в процессе обучения**.
Обычно в онлайн обучении с подкреплением агент взаимодействует с окружением, получает опыт (состояние, действие, вознаграждение и следующее состояние), обучается на нем (обновляет нейронную сеть) и отбрасывает его. Это неэффективно.

Воспроизведение опыта помогает **использовать опыт, полученный в ходе обучения, более эффективно**. Мы используем буфер воспроизведения, который сохраняет образцы опыта, **которые мы можем повторно использовать в процессе обучения**.
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay.jpg" alt="Воспроизведение опыта"/>

⇒ Это позволяет агенту **учиться на одном и том же опыте несколько раз**.

2. **Избежать забывания предыдущего опыта и уменьшить корреляцию между опытами**.
- Проблема, возникающая при последовательной выборке опыта для нейронной сети, заключается в том, что она склонна забывать **предыдущий опыт по мере получения нового**. Например, если агент побывал на первом уровне, а затем на втором, который отличается от первого, он может забыть, как вести себя и играть на первом уровне.

Решение состоит в том, чтобы создать буфер воспроизведения, который хранит кортежи опыта при взаимодействии с окружением, а затем сделать выборку небольшой партии кортежей. Это не позволяет **сети узнавать только о том, что она делала непосредственно перед этим*.

Повторение опыта имеет и другие преимущества. Случайная выборка опыта позволяет устранить корреляцию в последовательности наблюдений и избежать **колебания или катастрофического расхождения в парах "значений действий".**

В псевдокоде Глубокого Q-Обучения мы **инициализируем буфер воспроизведения памяти D емкостью N** (N - гиперпараметр, который вы можете определить). Затем мы сохраняем опыт в памяти и производим выборку батча опыта для подачи в глубокую Q-сеть на этапе обучения.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg" alt="Псевдокод повторения опыта"/>

## Фиксация Q-target для стабилизации обучения [[fixed-q]]

Когда мы хотим рассчитать ошибку TD (она же loss), мы рассчитываем **разницу между TD target (Q-Target) и текущей Q-value (оценкой Q)**.

Но мы **не имеем представления о реальной TD target**. Нам необходимо его оценить. Используя уравнение Беллмана, мы увидели, что TD target - это просто вознаграждение за выполнение данного действия в данном состоянии плюс дисконтированное наибольшее значение Q для следующего состояния.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg" alt="Q-target"/>

Однако проблема заключается в том, что мы используем одни и те же параметры (веса) для оценки TD target **и** Q-значения. Следовательно, существует значительная корреляция между TD target и параметрами, которые мы изменяем.

Поэтому на каждом шаге обучения **смещаются и наши Q-значения, и target значения.** Мы приближаемся к target, но target тоже движется. Это похоже на погоню за движущейся мишенью! Это может привести к значительным колебаниям в процессе обучения.

Это похоже на то, как если бы вы были ковбоем (оценка Q) и хотели поймать корову (Q-target). Ваша цель - приблизиться (уменьшить ошибку).

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-1.jpg" alt="Q-target"/>

На каждом временном шаге вы пытаетесь приблизиться к корове, которая также движется на каждом временном шаге (поскольку вы используете одни и те же параметры).

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-2.jpg" alt="Q-target"/>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-3.jpg" alt="Q-target"/>
This leads to a bizarre path of chasing (a significant oscillation in training).
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-4.jpg" alt="Q-target"/>

Вместо этого в псевдокоде мы видим, что:
- **Используем отдельную сеть с фиксированными параметрами** для оценки TD Target
- **Копируем параметры из нашей глубокой Q-сети каждые C шагов** для обновления целевой сети.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg" alt="Fixed Q-target Pseudocode"/>



## Двойная DQN [[double-dqn]]

Двойные DQN, или нейронные сети с двойным глубоким Q-Обучением, были представлены [Хадо ван Хасселтом] (https://papers.nips.cc/paper/3964-double-q-learning). Этот метод **решает проблему переоценки значений Q.**

Чтобы понять эту проблему, вспомним, как мы рассчитываем TD Target:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg" alt="TD target"/>

При вычислении TD target мы сталкиваемся с простой проблемой: как мы можем быть уверены, что **лучшим действием для следующего состояния является действие с наибольшим Q-значением?**

Мы знаем, что точность Q-значений зависит от того, какие действия мы пробовали **и** какие соседние состояния исследовали.

Следовательно, в начале обучения у нас нет достаточной информации о том, какое действие лучше предпринять. Поэтому принятие максимального значения Q (которое является зашумленным) в качестве наилучшего действия может привести к ложным срабатываниям. Если неоптимальным действиям регулярно **присваивать большее значение Q, чем оптимальному действию, то обучение будет затруднено.**

Решение заключается в следующем: при вычислении Q target мы используем две сети, чтобы отделить выбор действия от генерации Q-значения для target. Мы:
- Используем нашу **DQN-сеть** для выбора наилучшего действия для следующего состояния (действие с наибольшим Q-значением).
- Используем нашу **Целевую сеть (Target network)** для расчета target Q-value от выполнения этого действия в следующем состоянии.

Таким образом, Double DQN позволяет уменьшить переоценку Q-значений и, как следствие, быстрее и стабильнее обучаться.

После этих трех улучшений в Глубоком Q-Обучении появилось множество других, таких как Воспроизведение опыта с приоритетом (Prioritized Experience Replay) и Дуэльное Глубокое Q-Обучение (Dueling Deep Q-Learning). Они выходят за рамки данного курса, но если вы заинтересованы, посмотрите ссылки, которые мы поместили в список для дальнейшего изучения.
