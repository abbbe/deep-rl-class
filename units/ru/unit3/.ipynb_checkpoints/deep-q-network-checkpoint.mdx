# Глубокая Q-сеть (DQN)  [[deep-q-network]]
Это архитектура нашей глубокой нейронной сети с Q-Обучением:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg" alt="Глубокая Q Сеть"/>

В качестве входных данных мы принимаем **стек из 4 фреймов**, пропущенных через сеть, как состояние и выдаем **вектор Q-значений для каждого возможного действия в этом состоянии**. Затем, как и в случае с Q-Обучением, нам просто нужно использовать нашу эпсилон-жадную политику для выбора действия.

При инициализации нейросети **оценка Q-значения ужасна**. Но в процессе обучения наш агент с Глубоким Q-Обучением будет ассоциировать ситуацию с соответствующим действием и **научится хорошо играть в игру**.

## Предварительная обработка входных данных и временное ограничение [[preprocessing]]

Нам необходимо **предварительно обработать входные данные**. Это очень важный шаг, поскольку мы хотим **уменьшить сложность нашего состояния, чтобы сократить время вычислений, необходимое для обучения**.

Для этого мы **сокращаем пространство состояний до 84x84 и переводим его в серый цвет**. Мы можем это сделать, поскольку цвета в среде Atari не добавляют важной информации.
Это большое усовершенствование, поскольку мы **сокращаем наши три цветовых канала (RGB) до 1**.

В некоторых играх мы также можем **обрезать часть экрана**, если она не содержит важной информации.
Затем мы стэкируем четыре кадра вместе.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/preprocessing.jpg" alt="Предварительная обработка"/>

**Почему мы стэкируем четыре кадра вместе?
Мы стэкируем кадры вместе, потому что это помогает нам **справиться с проблемой временного ограничения**. Рассмотрим пример с игрой Pong. Когда вы видите этот кадр:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg" alt="Временное ограничение"/>

Можете ли вы сказать, куда летит мяч?
Нет, потому что одного кадра недостаточно для ощущения движения! Но что если добавить еще три кадра? **Здесь видно, что мяч движется вправо**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg" alt="Временное ограничение"/>
Поэтому для получения временной информации мы складываем четыре кадра вместе.

Затем стэкированные фреймы обрабатываются тремя сверточными слоями. Эти слои **позволяют нам улавливать и использовать пространственные отношения в изображениях**. Кроме того, поскольку фреймы стэкированны вместе, **мы можем использовать некоторые временные свойства этих фреймов**.

Если вы не знаете, что такое сверточные слои, не волнуйтесь. Вы можете ознакомиться с [Уроком 4 этого бесплатного курса по глубокому обучению от Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)

Наконец, у нас есть пара полносвязных слоев, которые возвращают Q-значение для каждого возможного действия в данном состоянии.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg" alt="Глубокая Q Сеть"/>

Таким образом, мы видим, что при Глубоком Q-Обучении нейронная сеть аппроксимирует, учитывая состояние, различные Q-значения для каждого возможного действия в этом состоянии. Теперь рассмотрим алгоритм Глубокого Q-Обучения.
