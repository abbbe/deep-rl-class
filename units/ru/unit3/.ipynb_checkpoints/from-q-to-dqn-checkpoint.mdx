# От Q-Обучения к Глубокому Q-Обучению [[from-q-to-dqn]]

Мы узнали, что **Q-Обучение - это алгоритм, который мы используем для обучения нашей Q-Функции**, **функции ценности действия**, определяющей ценность нахождения в определенном состоянии и выполнения определенного действия в этом состоянии.

<figure>
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg" alt="Q-функция"/>
</figure>

**Q происходит от "качества" данного действия в данном состоянии.**

Внутри наша Q-функция кодируется ** Q-таблицей - таблицей, каждая ячейка которой соответствует значению пары состояние-действие.** Рассматривайте эту Q-таблицу как **память или шпаргалку нашей Q-функции.**

Проблема заключается в том, что Q-Обучение - это *табличный метод*. Это становится проблемой, если пространства состояний и действий **не настолько малы, чтобы их можно было эффективно представить с помощью массивов и таблиц**. Другими словами, он **не масштабируется**.
Q-Обучение хорошо работало в средах с небольшим пространством состояний, таких как:

- FrozenLake, у нас было 16 состояний.
- Taxi-v3, у нас было 500 состояний.

Но подумайте о том, что мы собираемся сделать сегодня: мы будем обучать агента играть в Space Invaders, более сложную игру, используя фреймы в качестве входных данных.

Как **[упоминал Никита Мелкозеров](https://twitter.com/meln1k), в средах Atari** пространство наблюдений имеет форму (210, 160, 3)*, содержащую значения от 0 до 255, что дает \\(256^{210 \times 160 \times 3} = 256^{100800}\\) возможных наблюдений (для сравнения, в наблюдаемой Вселенной имеется примерно \\(10^{80}\\) атомов).

* Один кадр в Atari состоит из изображения размером 210x160 пикселей. Учитывая, что изображения являются цветными (RGB), в них имеется 3 канала. Поэтому форма имеет вид (210, 160, 3). Для каждого пикселя значение может быть от 0 до 255.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg" alt="Пространство состояний Atari"/>

Таким образом, пространство состояний гигантское, поэтому создание и обновление Q-таблицы для такой среды будет неэффективным. В этом случае оптимальным вариантом является аппроксимация Q-значений с помощью параметризованной Q-функции  \\(Q_{\theta}(s,a)\\)  .

Эта нейронная сеть будет аппроксимировать, учитывая состояние, различные Q-значения для каждого возможного действия в этом состоянии. Именно этим и занимается Глубокое Q-Обучение.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg" alt="Глубокое Q Обучение"/>


Теперь, когда мы разобрались с Глубоким Q-Обучением, давайте глубже погрузимся в Глубокую Q-Сеть.
