# Глоссарий 

Это глоссарий, созданный сообществом. Вклад в его создание приветствуется!

- Табличный метод:** Тип задачи, в которой пространства состояний и действий достаточно малы, чтобы аппроксимировать функции ценности действия, представляемые в виде массивов и таблиц. 
**Примером табличного метода является Q-Обучение**, поскольку для представления значений для различных пар "состояние-действие" используется таблица.

- Глубокое Q-Обучением:** Метод, который обучает нейронную сеть аппроксимировать, заданное состояние, различными **Q-значениями** для каждого возможного действия в этом состоянии.
Он используется для решения задач, когда пространство наблюдений слишком велико для применения табличного подхода Q-Обучения.  

- **Временное ограничение** - это трудность, возникающая, когда состояние среды представлено в виде фреймов. Сам по себе фрейм не дает временной информации. 
Для получения временной информации необходимо **стекировать** несколько кадров вместе.  

- **Фазы глубобокого Q-Обучения:**
  - **Выборка (Сэмплирование, Sampling):** Выполняются действия, и наблюдаемые кортежи опыта сохраняются в **памяти воспроизведения**.
  - **Обучение:** Батчи кортежей выбираются случайным образом, и нейронная сеть обновляет свои веса с помощью градиентного спуска. 
  
- **Решения для стабилизации глубобокого Q-Обучения:**.
  - **Воспроизведение опыта:** Создается память воспроизведения для сохранения примеров опыта, которые могут быть повторно использованы в процессе обучения. 
  Это позволяет агенту обучаться на одном и том же опыте несколько раз. Кроме того, это помогает агенту не забывать предыдущий опыт по мере получения нового.
  - **Случайная выборка (Случайный сэплинг, Random sampling)** из буфера воспроизведения позволяет устранить корреляцию в последовательностях наблюдений и предотвратить катастрофические колебания или расхождения значений действий.

  - **Фиксированный Q-Target:** Для расчета **Q-Target** необходимо оценить дисконтированное оптимальное **Q-значение** следующего состояния с помощью уравнения Беллмана. Проблема
  заключается в том, что для расчета **Q-Target** и **Q-значения** используются одни и те же веса сети. Это означает, что каждый раз, когда мы изменяем **Q-значение**, вместе с ним изменяется и **Q-Target**.
  Чтобы избежать этой проблемы, для оценки Target временных различий используется отдельная сеть с фиксированными параметрами. Target-сеть обновляется путем копирования параметров из нашей Глубокой Q-Сети
  после определенного количества **C шага**. 
  
  - **Двойной DQN:** Метод обработки **переоценки** **Q-значений**. Это решение использует две сети для разделения выбора действия и генерации целевого (target) **значения**:
     - **Сеть DQN** выбирает наилучшее действие для следующего состояния (действие с наибольшим **Q-значением**)
     - **Целевая сеть (Target Network)** рассчитывает целевое (target) **Q-значение** выполнения этого действия в следующем состоянии. 
  Такой подход уменьшает завышение **Q-значений**, помогает быстрее обучаться и обеспечивает более стабильное обучение.

Если вы хотите улучшить курс, вы можете [открыть Pull Request].(https://github.com/huggingface/deep-rl-class/pulls)

Создание данного глоссария стало возможным благодаря:

- [Dario Paez](https://github.com/dario248)
