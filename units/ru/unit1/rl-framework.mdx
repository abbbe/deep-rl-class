# Концепция обучения с подкреплением [[the-reinforcement-learning-framework]]

## Процесс RL [[the-rl-process]]

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg" alt="The RL process" width="100%">
<figcaption>The RL Process: a loop of state, action, reward and next state</figcaption>
<figcaption>Source: <a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto</a></figcaption>
</figure>

Для понимания процесса RL представим себе агента, обучающегося играть в платформенную игру:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg" alt="The RL process" width="100%">

- Наш агент получает **состояние  \\(S_0\\)** от **среды** - мы получаем первый кадр нашей игры (среда).
- На основании этого **состояния \\\(S_0\\\),** агент предпринимает **действие \\\(A_0\\)** - наш агент перемещается вправо.
- Среда переходит в **новое** **состояние \\\(S_1\\\)** - новый кадр.
- Среда дает некоторое **вознаграждение \\\(R_1\\\)** агенту - мы не умерли *(Положительное вознаграждение +1)*.

Этот цикл RL выводит последовательность **состояние, действие, вознаграждение и следующее состояние**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg" alt="State, Action, Reward, Next State" width="100%">

Цель агента состоит в том, чтобы _максимизировать_ свое кумулятивное вознаграждение, **называемое ожидаемой доходностью**.

## Гипотеза вознаграждения: центральная идея обучения с подкреплением [[reward-hypothesis]]

⇒ Почему целью агента является максимизация ожидаемой доходности?

Потому что в основе RL лежит **гипотеза вознаграждения**, согласно которой все цели могут быть описаны как **максимизация ожидаемой доходности** (ожидаемого кумулятивного вознаграждения).

Именно поэтому в обучении с подкреплением, **чтобы добиться наилучшего поведения**, мы стремимся научиться выполнять действия, которые **максимально увеличивают ожидаемое кумулятивное вознаграждение**.


## Марковское свойство [[markov-property]]

В статьях вы увидите, что процесс RL называется **Марковским процессом принятия решений** (Markov Decision Process, MDP).

Мы еще поговорим о свойстве Маркова в следующих разделах. Но если вам нужно что-то запомнить о нем сегодня, то это следующее: свойство Маркова подразумевает, что нашему агенту для принятия решения** о том, какое действие предпринять, нужно **только текущее состояние, а **не история всех состояний и действий**, которые он предпринимал до этого.

## Наблюдения/Пространство состояний [[obs-space]]

Наблюдения/состояния - это **информация, которую наш агент получает из среды.** В случае видеоигры это может быть кадр (снимок экрана). В случае торгового агента это может быть стоимость определенной акции и т.д.

Однако здесь необходимо провести различие между *наблюдением* и *состоянием*:

- *Состояние s*: является **полным описанием состояния мира** (скрытой информации нет). В полностью наблюдаемой среде.


<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg" alt="Chess">
<figcaption>В шахматной игре мы получаем состояние из среды, поскольку имеем доступ ко всей информации о шахматной доске.</figcaption>
</figure>

В шахматной игре мы имеем доступ к информации обо всей доске, поэтому получаем состояние среды. Другими словами, среда полностью наблюдаема.

- *Наблюдение o*: является **частичным описанием состояния.** В частично наблюдаемой среде.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" alt="Mario">
<figcaption>В Super Mario Bros мы видим только ту часть уровня, которая находится в непосредственной близости от игрока, поэтому получаем наблюдение.</figcaption>
</figure>

В Super Mario Bros мы видим только ту часть уровня, которая находится в непосредственной близости от игрока, поэтому получаем наблюдение.

В Super Mario Bros мы находимся в частично наблюдаемой среде. Мы получаем наблюдение **поскольку видим только часть уровня**.

<Tip>
В этом курсе мы используем термин "состояние (state)" для обозначения как состояния, так и наблюдения, но в реализациях мы будем делать различие.
</Tip>

Напомним:
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg" alt="Obs space recap" width="100%">


## Пространство действий [[action-space]]

Пространство действий (Action space) - это множество **всех возможных действий в среде**.

Действия могут исходить из *дискретного* или *непрерывного пространства*:

- *Дискретное пространство*: число возможных действий **конечно**.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" alt="Mario">
<figcaption>В Super Mario Bros у нас есть только 4 возможных действия: влево, вправо, вверх (прыжок) и вниз (приседание).</figcaption>

</figure>

Опять же, в Super Mario Bros мы имеем конечный набор действий, поскольку у нас всего 4 направления.

- *Непрерывное пространство*: число возможных действий **бесконечно**.

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg" alt="Self Driving Car">
<figcaption>Агент автономного автомобиля имеет бесконечное число возможных действий, поскольку он может поворачивать налево на 20°, 21,1°, 21,2°, сигналить, поворачивать направо на 20°...
</figcaption>
</figure>

Напомним:
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg" alt="Action space recap" width="100%">

Учесть эту информацию крайне важно, поскольку она **будет иметь значение при выборе алгоритма RL в будущем**.

## Вознаграждение и дисконтирование [[rewards]]

Вознаграждение является основополагающим фактором в RL, поскольку это **единственная обратная связь** для агента. Благодаря ей наш агент знает, **хорошо ли он поступил или нет*.

Кумулятивное вознаграждение на каждом временном шаге **t** можно записать в виде::

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg" alt="Rewards">
<figcaption>Кумулятивное вознаграждение равно сумме всех вознаграждений в последовательности.
</figcaption>
</figure>

Что эквивалентно:

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg" alt="Rewards">
<figcaption>Кумулятивное вознаграждение = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1 = rt+2) + ...
</figcaption>
</figure>

Однако в реальности **мы не можем просто так сложить их.** Награды, которые приходят раньше (в начале игры) **более вероятны**, поскольку они более предсказуемы, чем долгосрочные будущие награды.

Допустим, ваш агент - крошечная мышка, которая может перемещаться на одну плитку каждый шаг времени, а ваш противник - кошка (которая тоже может перемещаться). Задача мыши - **съесть максимальное количество сыра, прежде чем ее съест кошка**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg" alt="Rewards" width="100%">

Как видно из диаграммы, **вероятность съесть сыр рядом с нами выше, чем сыр рядом с кошкой** (чем ближе мы к кошке, тем она опаснее).

Следовательно, **вознаграждение рядом с кошкой, даже если оно больше (больше сыра), будет более уцененным**, поскольку мы не совсем уверены, что сможем его съесть.

Для дисконтирования вознаграждений мы поступаем следующим образом:

1. Мы определяем коэффициент дисконтирования, называемый гаммой. **Он должен находиться в диапазоне от 0 до 1.** Чаще всего он находится в диапазоне от **0,95 до 0,99**.
- Чем больше гамма, тем меньше дисконт. Это означает, что наш агент **заботится о долгосрочном вознаграждении**.
- С другой стороны, чем меньше гамма, тем больше дисконт. Это означает, что наш **агент больше заботится о краткосрочном вознаграждении (ближайшем сыре)**.

2. Затем каждое вознаграждение дисконтируется на гамму, равную экспоненте временного шага. С увеличением временного шага кошка приближается к нам, **поэтому вероятность того, что будущее вознаграждение произойдет, становится все меньше и меньше**.

Наше дисконтированное ожидаемое кумулятивное вознаграждение составляет:
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg" alt="Rewards" width="100%">
