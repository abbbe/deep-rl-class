# Краткое содержание [[summary]]
Это был большой объем информации! Давайте подведем итоги:

- Обучение с подкреплением - это вычислительный подход к обучению на основе действий. Мы создаем агента, который учится у среды **взаимодействуя с ней методом проб и ошибок** и получая вознаграждение (отрицательное или положительное) в качестве обратной связи.

- Целью любого агента RL является максимизация его ожидаемого кумулятивного вознаграждения (также называемого ожидаемой доходностью), поскольку RL основана на **гипотезе вознаграждения**, согласно которой **все цели могут быть описаны как максимизация ожидаемого кумулятивного вознаграждения.**

- Процесс RL представляет собой цикл, который выводит последовательность **состояние, действие, вознаграждение и следующее состояние.**

- Для расчета ожидаемого кумулятивного вознаграждения (ожидаемой доходности) мы дисконтируем вознаграждения: вознаграждения, которые получены раньше (в начале игры) **более вероятны, так как они более предсказуемы, чем долгосрочное будущее вознаграждение.**

- Для решения задачи RL необходимо **найти оптимальную политику**. Политика - это "мозг" вашего агента, который говорит нам, **какие действия следует предпринять в том или ином состоянии.** Оптимальная политика - это та, которая **дает вам действия, максимизирующие ожидаемую доходность.**

- Существует два способа найти оптимальную политику:
    1. Непосредственно обучая свою политику: **методы, основанные на политике.**
    2. Обучение функции стоимости, которая говорит нам об ожидаемой доходности агента в каждом состоянии, и использование этой функции для определения нашей политики: **методы, основанные на ценности.**

- Наконец, мы говорим о глубоком RL (Deep Reinforcement learning), поскольку используем **глубокие нейронные сети для оценки действий (policy-based) или для оценки ценности состояния (value-based)**, отсюда и название "глубокое".
